import os
import time
import random
import uuid
import sqlite3
import requests
import concurrent.futures
from bs4 import BeautifulSoup
from datetime import datetime, timezone
import xml.etree.ElementTree as ET
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# === –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ===
SITEMAP_URL = "https://www.newsvl.ru/sitemap_vl_news.xml"
DB_FILE = "vladivostok_news2.db"
MAX_ARTICLES = 5000  # –¶–µ–ª–µ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π
MAX_WORKERS = 5  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞

USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 YaBrowser/25.6.1.1000 Yowser/2.5 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Safari/605.1.15",
]

REQUEST_DELAY = (0.3, 0.8)  # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
RETRY_ATTEMPTS = 3


# === 1. –°–æ–∑–¥–∞—ë–º –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π ===
def init_db():
    conn = sqlite3.connect(DB_FILE)
    cur = conn.cursor()

    # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—É—é —Ç–∞–±–ª–∏—Ü—É –µ—Å–ª–∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç (–¥–ª—è —á–∏—Å—Ç–æ–≥–æ –∑–∞–ø—É—Å–∫–∞)
    cur.execute("DROP TABLE IF EXISTS articles")

    cur.execute("""
        CREATE TABLE articles (
            guid TEXT PRIMARY KEY,
            title TEXT NOT NULL,
            description TEXT NOT NULL,
            url TEXT NOT NULL UNIQUE,
            published_at TEXT,
            comments_count INTEGER DEFAULT 0,
            created_at_utc TEXT NOT NULL,
            rating INTEGER,
            word_count INTEGER GENERATED ALWAYS AS (LENGTH(description) - LENGTH(REPLACE(description, ' ', '')) + 1) VIRTUAL
        )
    """)

    # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–π —Ä–∞–±–æ—Ç—ã
    cur.execute("CREATE INDEX idx_url ON articles(url)")
    cur.execute("CREATE INDEX idx_published ON articles(published_at)")
    cur.execute("CREATE INDEX idx_created ON articles(created_at_utc)")

    conn.commit()
    conn.close()
    print(f"–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö '{DB_FILE}' —Å–æ–∑–¥–∞–Ω–∞ —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏")


# === 2. –£–ª—É—á—à–µ–Ω–Ω–∞—è —Å–µ—Å—Å–∏—è requests —Å –ø–æ–≤—Ç–æ—Ä–∞–º–∏ ===
def create_session():
    session = requests.Session()

    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –ø—Ä–æ–≥—Ä–∞–º–º—É –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü. –î–µ–ª–∞–µ–º –≤–µ–∂–ª–∏–≤—ã–º,
    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø–æ–≤—Ç–æ—Ä–Ω—ã–µ –ø–æ–ø—ã—Ç–∫–∏, –µ—Å–ª–∏ —Å–∞–π—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞–∑–Ω—ã–µ User Agent, —á—Ç–æ–±—ã –Ω–µ –∑–∞–±–ª–æ–∫–∞–ª–∏
    retry_strategy = Retry(
        total=RETRY_ATTEMPTS,
        backoff_factor=0.5,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["GET"]
    )
    adapter = HTTPAdapter(max_retries=retry_strategy, pool_connections=20, pool_maxsize=20)
    session.mount("http://", adapter)
    session.mount("https://", adapter)

    return session


# === 3. –ü–æ–ª—É—á–∞–µ–º URL –∏–∑ sitemap —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π ===
# === –° –ø–æ–º–æ—â—å—é sitemap.xml –∑–∞–≥—Ä—É–∂–∞–µ—Ç –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ —Å–∞–π—Ç–∞ ===
def fetch_sitemap_urls(session, limit=7000):
    """–ü–æ–ª—É—á–∞–µ–º URL –∏–∑ sitemap"""
    print("–ó–∞–≥—Ä—É–∂–∞–µ–º sitemap...")
    headers = {"User-Agent": random.choice(USER_AGENTS)}

    try:
        resp = session.get(SITEMAP_URL, headers=headers, timeout=15)
        resp.raise_for_status()
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ sitemap: {e}")
        return []

    root = ET.fromstring(resp.content)
    urls_with_dates = []

    # –°–æ–±–∏—Ä–∞–µ–º URL –∏ –¥–∞—Ç—ã –¥–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏
    for url_tag in root.findall(".//{http://www.sitemaps.org/schemas/sitemap/0.9}url"):
        loc = url_tag.find("{http://www.sitemaps.org/schemas/sitemap/0.9}loc")
        lastmod = url_tag.find("{http://www.sitemaps.org/schemas/sitemap/0.9}lastmod")

        if loc is not None:
            url = loc.text.strip()
            date = lastmod.text if lastmod is not None else ""
            urls_with_dates.append((url, date))

    print(f"–ù–∞–π–¥–µ–Ω–æ {len(urls_with_dates)} –∑–∞–ø–∏—Å–µ–π –≤ sitemap")

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ (—Å–≤–µ–∂–∏–µ —Å–Ω–∞—á–∞–ª–∞) –∏ –±–µ—Ä–µ–º –Ω—É–∂–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
    urls_with_dates.sort(key=lambda x: x[1], reverse=True)
    urls = [url for url, date in urls_with_dates[:limit]]

    print(f"–û—Ç–æ–±—Ä–∞–Ω–æ {len(urls)} —Å–∞–º—ã—Ö —Å–≤–µ–∂–∏—Ö URL")
    return urls


# === 4. –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ ===
def clean_article_text(soup):
    """–ò–∑–≤–ª–µ–∫–∞–µ–º –∏ –æ—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏"""
    # –û—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç
    text_block = soup.find("div", class_="story__text")
    if not text_block:
        # –ü—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
        text_block = soup.find("div", class_=lambda x: x and ("article" in x or "content" in x or "text" in x))
        if not text_block:
            return None

    # –ö–ª–æ–Ω–∏—Ä—É–µ–º –±–ª–æ–∫ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ —É–¥–∞–ª–µ–Ω–∏—è —ç–ª–µ–º–µ–Ω—Ç–æ–≤
    text_block = BeautifulSoup(str(text_block), 'html.parser')

    # –£–¥–∞–ª—è–µ–º –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
    unwanted_selectors = [
        "img", "video", "audio", "iframe", "script", "style",
        "figure", ".embed-responsive", ".social-share", ".advertisement",
        ".banner", "ins", ".ya-share2", ".teaser", "[data-type='ad']"
    ]

    for selector in unwanted_selectors:
        for element in text_block.select(selector):
            element.decompose()

    # –£–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—Å—Ç
    for a in text_block.find_all("a"):
        a.replace_with(a.get_text())

    # –£–±–∏—Ä–∞–µ–º –ø—É—Å—Ç—ã–µ —Ç–µ–≥–∏
    for tag in text_block.find_all():
        if not tag.get_text(strip=True) and not tag.attrs:
            tag.decompose()

    # –ü–æ–ª—É—á–∞–µ–º —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç
    text = text_block.get_text(separator="\n", strip=True)

    # –û—á–∏—Å—Ç–∫–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫
    lines = [line.strip() for line in text.split("\n") if line.strip()]
    cleaned_text = "\n".join(lines)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É
    if len(cleaned_text) < 300:  # –ú–∏–Ω–∏–º—É–º 300 —Å–∏–º–≤–æ–ª–æ–≤
        return None

    return cleaned_text


# === 5. –ü–∞—Ä—Å–∏–º –æ–¥–Ω—É —Å—Ç–∞—Ç—å—é ===
def parse_article(url, session):
    """–ü–∞—Ä—Å–∏–º —Å—Ç–∞—Ç—å—é —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
    headers = {"User-Agent": random.choice(USER_AGENTS)}

    try:
        resp = session.get(url, headers=headers, timeout=10)

        if resp.status_code != 200:
            if resp.status_code == 404:
                return {"status": "skipped", "reason": "404 Not Found"}
            elif resp.status_code == 403:
                return {"status": "error", "reason": "403 Forbidden"}
            else:
                return {"status": "error", "reason": f"HTTP {resp.status_code}"}

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ HTML —Å—Ç—Ä–∞–Ω–∏—Ü–∞
        if 'text/html' not in resp.headers.get('content-type', '').lower():
            return {"status": "skipped", "reason": "Not HTML"}

        soup = BeautifulSoup(resp.content, "html.parser", from_encoding='utf-8')

        # –ó–∞–≥–æ–ª–æ–≤–æ–∫
        title_elem = soup.find("h1", class_="story__title")
        if not title_elem:
            title_elem = soup.find("h1")

        title = title_elem.get_text(strip=True) if title_elem else "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"

        # –¢–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏
        description = clean_article_text(soup)
        if not description:
            return {"status": "skipped", "reason": "No text content"}

        # –î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
        published_at = None

        # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –¥–∞—Ç—É –≤ –º–µ—Ç–∞-—Ç–µ–≥–∞—Ö
        meta_date = soup.find("meta", property="article:published_time") or \
                    soup.find("meta", property="og:published_time") or \
                    soup.find("meta", attrs={"name": "pubdate"})

        if meta_date and meta_date.get("content"):
            published_at = meta_date["content"]
        else:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑ URL
            import re
            match = re.search(r"/(\d{4})/(\d{2})/(\d{2})/", url)
            if match:
                year, month, day = match.groups()
                published_at = f"{year}-{month}-{day} 00:00:00"
            else:
                # –ò—â–µ–º –¥–∞—Ç—É –≤ —Ç–µ–∫—Å—Ç–µ
                date_patterns = [
                    r'\d{1,2}\s+[–∞-—è]+\s+\d{4}',
                    r'\d{1,2}\.\d{1,2}\.\d{4}'
                ]
                for pattern in date_patterns:
                    match = re.search(pattern, soup.get_text()[:500])
                    if match:
                        published_at = match.group(0)
                        break

        # –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
        comments_count = 0
        comments_elem = soup.find(class_=lambda x: x and ("comment" in x.lower() or "–∫–æ–º–º–µ–Ω—Ç" in x.lower()))
        if comments_elem:
            import re
            numbers = re.findall(r'\d+', comments_elem.get_text())
            if numbers:
                comments_count = int(numbers[0])

        return {
            "status": "success",
            "data": {
                "guid": str(uuid.uuid4()),
                "title": title[:500],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
                "description": description,
                "url": url,
                "published_at": published_at,
                "comments_count": comments_count,
                "created_at_utc": datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S"),
                "rating": None
            }
        }

    except requests.exceptions.Timeout:
        return {"status": "error", "reason": "Timeout"}
    except requests.exceptions.RequestException as e:
        return {"status": "error", "reason": str(e)}
    except Exception as e:
        return {"status": "error", "reason": f"Parse error: {str(e)}"}


# === 6. –ü–∞–∫–µ—Ç–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î  ===
class DatabaseBatchSaver:
    def __init__(self, db_file, batch_size=100):
        self.db_file = db_file
        self.batch_size = batch_size
        self.batch = []
        self.total_saved = 0

    def add_article(self, article):
        self.batch.append(article)
        if len(self.batch) >= self.batch_size:
            self.flush()

    def flush(self):
        if not self.batch:
            return

        conn = sqlite3.connect(self.db_file)
        cur = conn.cursor()

        try:
            cur.executemany("""
                INSERT OR IGNORE INTO articles
                (guid, title, description, url, published_at, comments_count, created_at_utc, rating)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """, [
                (
                    article["guid"],
                    article["title"],
                    article["description"],
                    article["url"],
                    article["published_at"],
                    article["comments_count"],
                    article["created_at_utc"],
                    article["rating"]
                )
                for article in self.batch
            ])

            saved_count = cur.rowcount
            self.total_saved += saved_count
            conn.commit()

            print(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –ø–∞–∫–µ—Ç–æ–º: {saved_count} —Å—Ç–∞—Ç–µ–π (–≤—Å–µ–≥–æ: {self.total_saved})")

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞–∫–µ—Ç–Ω–æ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏: {e}")
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ –æ–¥–Ω–æ–π –ø—Ä–∏ –æ—à–∏–±–∫–µ
            for article in self.batch:
                try:
                    cur.execute("""
                        INSERT OR IGNORE INTO articles
                        (guid, title, description, url, published_at, comments_count, created_at_utc, rating)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                    """, tuple(article.values()))
                except:
                    continue
            conn.commit()
        finally:
            conn.close()

        self.batch.clear()

    def close(self):
        self.flush()
        return self.total_saved


# === 7. –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å—Ç–∞—Ç–µ–π ===
def parse_articles_parallel(urls, max_articles=MAX_ARTICLES):
    """–ü–∞—Ä—Å–∏–º —Å—Ç–∞—Ç—å–∏ –≤ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ—Ç–æ–∫–æ–≤"""
    print(f"–ó–∞–ø—É—Å–∫ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ ({MAX_WORKERS} –ø–æ—Ç–æ–∫–æ–≤)...")

    db_saver = DatabaseBatchSaver(DB_FILE, batch_size=50)
    session = create_session()

    stats = {
        "total": 0,
        "success": 0,
        "skipped": 0,
        "errors": 0,
        "start_time": time.time()
    }

    def process_url(url):
        nonlocal stats
        stats["total"] += 1

        # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
        if stats["total"] % 100 == 0:
            elapsed = time.time() - stats["start_time"]
            speed = stats["total"] / elapsed if elapsed > 0 else 0
            print(f"–ü—Ä–æ–≥—Ä–µ—Å—Å: {stats['total']} | –£—Å–ø–µ—à–Ω–æ: {stats['success']} | "
                  f"–°–∫–æ—Ä–æ—Å—Ç—å: {speed:.1f} —Å—Ç–∞—Ç–µ–π/—Å–µ–∫ | "
                  f"–í—Ä–µ–º—è: {elapsed / 60:.1f} –º–∏–Ω")

        # –ó–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
        time.sleep(random.uniform(*REQUEST_DELAY))

        result = parse_article(url, session)

        if result["status"] == "success":
            db_saver.add_article(result["data"])
            stats["success"] += 1
            return "success"
        elif result["status"] == "skipped":
            stats["skipped"] += 1
            return "skipped"
        else:
            stats["errors"] += 1
            return "error"

    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
    urls_to_parse = urls[:max_articles * 2]  # –ë–µ—Ä–µ–º —Å –∑–∞–ø–∞—Å–æ–º

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º ThreadPoolExecutor –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {executor.submit(process_url, url): url for url in urls_to_parse}

        completed = 0
        for future in concurrent.futures.as_completed(futures):
            completed += 1

            # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è –µ—Å–ª–∏ —Å–æ–±—Ä–∞–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å—Ç–∞—Ç–µ–π
            if stats["success"] >= max_articles:
                print(f"–î–æ—Å—Ç–∏–≥–Ω—É—Ç–∞ —Ü–µ–ª—å: {max_articles} —Å—Ç–∞—Ç–µ–π!")
                executor.shutdown(wait=False, cancel_futures=True)
                break

            # –ü—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 50 —Å—Ç–∞—Ç–µ–π
            if completed % 50 == 0:
                print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {completed}/{len(urls_to_parse)} | "
                      f"–°–æ–±—Ä–∞–Ω–æ: {stats['success']}/{max_articles}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è —Å—Ç–∞—Ç—å–∏
    total_saved = db_saver.close()

    # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    elapsed = time.time() - stats["start_time"]
    print(f"\n{'=' * 60}")
    print("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–ê–†–°–ò–ù–ì–ê")
    print(f"{'=' * 60}")
    print(f"–£—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {total_saved} —Å—Ç–∞—Ç–µ–π")
    print(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ: {stats['skipped']} —Å—Ç–∞—Ç–µ–π")
    print(f"–û—à–∏–±–æ–∫: {stats['errors']}")
    print(f"–û–±—â–µ–µ –≤—Ä–µ–º—è: {elapsed / 60:.1f} –º–∏–Ω—É—Ç")
    print(f"–°–∫–æ—Ä–æ—Å—Ç—å: {stats['total'] / elapsed:.1f} —Å—Ç–∞—Ç–µ–π/—Å–µ–∫")
    print(f"{'=' * 60}")

    session.close()
    return total_saved


# === 8. –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö ===
def check_database():
    """–ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –±–∞–∑—ã"""
    conn = sqlite3.connect(DB_FILE)
    cur = conn.cursor()

    # –û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    cur.execute("SELECT COUNT(*) FROM articles")
    total = cur.fetchone()[0]

    cur.execute("""
        SELECT 
            MIN(published_at) as earliest,
            MAX(published_at) as latest,
            COUNT(DISTINCT DATE(published_at)) as days_count
        FROM articles 
        WHERE published_at IS NOT NULL
    """)

    stats = cur.fetchone()

    cur.execute("""
        SELECT title, url, published_at, LENGTH(description) as length
        FROM articles 
        ORDER BY RANDOM() 
        LIMIT 3
    """)

    samples = cur.fetchall()

    conn.close()

    print(f"\n–ü–†–û–í–ï–†–ö–ê –ë–ê–ó–´ –î–ê–ù–ù–´–•:")
    print(f"   –í—Å–µ–≥–æ —Å—Ç–∞—Ç–µ–π: {total}")
    if stats[0]:
        print(f"   –ü–µ—Ä–≤–∞—è —Å—Ç–∞—Ç—å—è: {stats[0]}")
        print(f"   –ü–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç–∞—Ç—å—è: {stats[1]}")
        print(f"   –°—Ç–∞—Ç–µ–π –∑–∞ –¥–Ω–µ–π: {stats[2]}")

    print(f"\n–ü—Ä–∏–º–µ—Ä—ã —Å—Ç–∞—Ç–µ–π:")
    for i, (title, url, date, length) in enumerate(samples, 1):
        print(f"   {i}. {title[:60]}...")
        print(f"      URL: {url[:50]}...")
        print(f"      –î–∞—Ç–∞: {date[:10] if date else '–ù–µ—Ç'}")
        print(f"      –î–ª–∏–Ω–∞: {length} —Å–∏–º–≤–æ–ª–æ–≤")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
    if os.path.exists(DB_FILE):
        size_mb = os.path.getsize(DB_FILE) / (1024 * 1024)
        print(f"\n –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –ë–î: {size_mb:.2f} MB")

    return total


# === 9. –û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—É—Å–∫ ===
def main():
    print("=" * 60)
    print("–ü–ê–†–°–ï–† VLADIVOSTOK NEWS")
    print("=" * 60)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ë–î
    init_db()

    # –°–æ–∑–¥–∞–µ–º —Å–µ—Å—Å–∏—é –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ sitemap
    session = create_session()

    # –ü–æ–ª—É—á–∞–µ–º URL
    urls = fetch_sitemap_urls(session, limit=7000)

    if not urls:
        print("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å URL. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã.")
        return

    print(f"–¶–µ–ª—å: —Å–æ–±—Ä–∞—Ç—å {MAX_ARTICLES} —Å—Ç–∞—Ç–µ–π")
    print(f"–î–æ—Å—Ç—É–ø–Ω–æ URL: {len(urls)}")

    # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Å–µ—Å—Å–∏—é sitemap
    session.close()

    # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥
    total_saved = parse_articles_parallel(urls, MAX_ARTICLES)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    check_database()

    if total_saved >= MAX_ARTICLES:
        print(f"\n –£–°–ü–ï–•! –°–æ–±—Ä–∞–Ω–æ {total_saved} —Å—Ç–∞—Ç–µ–π (—Ü–µ–ª—å: {MAX_ARTICLES})")
        print(f"–§–∞–π–ª –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö: {DB_FILE}")
    else:
        print(f"\n –°–æ–±—Ä–∞–Ω–æ —Ç–æ–ª—å–∫–æ {total_saved} —Å—Ç–∞—Ç–µ–π –∏–∑ {MAX_ARTICLES}")
        print("   –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–≤–µ–ª–∏—á–∏—Ç—å MAX_WORKERS –∏–ª–∏ —É–º–µ–Ω—å—à–∏—Ç—å REQUEST_DELAY")


# === 10. –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç ===
def quick_test():
    """–ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç—ã –ø–∞—Ä—Å–µ—Ä–∞"""
    print("üîß –¢–ï–°–¢–û–í–´–ô –†–ï–ñ–ò–ú (—Å–æ–±–µ—Ä–µ—Ç 20 —Å—Ç–∞—Ç–µ–π)")

    global MAX_ARTICLES
    MAX_ARTICLES = 20

    init_db()

    session = create_session()
    urls = fetch_sitemap_urls(session, limit=50)
    session.close()

    if urls:
        # –ü–∞—Ä—Å–∏–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –¥–ª—è —Ç–µ—Å—Ç–∞
        db_saver = DatabaseBatchSaver(DB_FILE, batch_size=10)

        for i, url in enumerate(urls[:30], 1):
            if db_saver.total_saved >= 20:
                break

            print(f"[{i}] –¢–µ—Å—Ç: {url[:60]}...")
            result = parse_article(url, create_session())

            if result["status"] == "success":
                db_saver.add_article(result["data"])
                print(f"   –£—Å–ø–µ—Ö")
            else:
                print(f"   –ü—Ä–æ–ø—É—â–µ–Ω–æ: {result['reason']}")

            time.sleep(0.5)

        db_saver.close()
        check_database()


if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1 and sys.argv[1] == "test":
        quick_test()
    else:
        main()